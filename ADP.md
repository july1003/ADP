# 📕 ADP 필기 합격 통합 마스터 가이드

데이터분석 전문가 자격검정 시험 과목 구성

데이터분석 전문가 자격검정 시험의 과목은 총 5과목으로 구성되어 있으며, 데이터 이해 과목을 바탕으로 데이터 처리 기술 이해, 데이터분석 기획, 데이터분석, 데이터 시각화를 수행하는 능력을 검정한다.

| 과목명                | 주요항목                                               | 세부항목                                                     |
| --------------------- | ------------------------------------------------------ | ------------------------------------------------------------ |
| 데이터 이해           | 데이터의 이해                                          | - 데이터와 정보<br>- 데이터베이스의 정의와 특징<br>- 데이터베이스 활용<br>- 빅데이터 이해 |
| 데이터 이해           | 데이터의 가치와 미래                                   | - 빅데이터의 가치와 영향<br>- 비즈니스 모델<br>- 위기 요인과 통제 방안<br>- 미래의 빅데이터<br>- 빅데이터 분석과 전략 인사이트 |
| 데이터 이해           | 가치 창출을 위한 <br />데이터 사이언스와 전략 인사이트 | - 전략적 인사이트 도출을 위한 데이터 분석<br>- 데이터 과학의 이해와 미래 전망 |
| 데이터 처리 기술 이해 | 데이터 처리 프로세스                                   | - ETL(Extraction, Transformation and Load)<br>- CDC(Change Data Capture)<br>- EAI(Enterprise Application Integration)<br>- 데이터 연계 및 통합 기법 요약<br>- 대용량 비정형 데이터 처리 |
| 데이터 처리 기술 이해 | 데이터 처리 기술                                       | - 분석 데이터 저장 기술<br>- 분산 컴퓨팅 기술<br>- 클라우드 인프라 기술 |
| 데이터분석 기획       | 데이터분석 기획의 이해                                 | - 분석 기획 방향성 도출<br>- 분석 방법론<br>- 분석 과제 발굴<br>- 분석 프로젝트 관리 방안 |
| 데이터분석 기획       | 분석 마스터 플랜                                       | - 마스터 플랜 수립<br>- 분석 거버넌스 체계 수립              |
| 데이터분석            | R기초와 데이터 마트                                    | - R기초<br>- 데이터 마트<br>- 결측값 처리와 이상값 검색      |
| 데이터분석            | 통계분석                                               | - 통계학 개론<br>- 통계 기초분석<br>- 다변량 분석<br>- 시계열 예측 |
| 데이터분석            | 정형 데이터 마이닝                                     | - 데이터 마이닝 개요<br>- 분류분석(Classification)<br>- 군집분석(Clustering)<br>- 연관분석(Association Analysis) |
| 데이터분석            | 비정형 데이터 마이닝                                   | - 텍스트 마이닝<br>- 사회연결망 분석                         |
| 데이터 시각화         | 시각화 인사이트 프로세스                               | - 시각화 인사이트 프로세스의 의미<br>- 탐색(1단계)<br>- 분석(2단계)<br>- 활용(3단계) |
| 데이터 시각화         | 시각화 디자인                                          | - 시각화 정의<br>- 시각화 프로세스<br>- 시각화 방법<br>- 빅데이터와 시각화 디자인 |
| 데이터 시각화         | 시각화 구현                                            | - 시각화 구현 개요<br>- 분석 도구를 이용한 시각화 구현<br>- 라이브러리 기반의 시각화 구현: D3.js |

------

# 1. 데이터 이해 (Detailed Study Guide)

## 1.1 데이터의 이해

### 1.1.1 데이터와 정보 (Data & Information)

- **① 개념 정의**
  - **데이터(Data):** 객관적 사실(Fact)로서 그 자체로는 의미가 적은 수치나 기호.
  - **정보(Information):** 데이터를 특정 목적에 맞게 처리, 가공하여 의사결정에 활용 가능한 형태로 만든 것.
- **② DIKW 피라미드 (상세 구조)**
  1. **Data (데이터):** 가공 전의 순수한 사실 (예: A마트 연필 가격 100원).
  2. **Information (정보):** 데이터 간의 관계 속에서 의미 도출 (예: 연필 가격이 지우개보다 저렴함).
  3. **Knowledge (지식):** 정보를 체계화하여 내재화한 결과 (예: 연필은 저가형 필기구에 속함).
  4. **Wisdom (지혜):** 지식에 근거한 유연한 판단과 통찰 (예: 경기 불황기에는 저가형 필기구인 연필의 재고를 늘려야 함).
- **③ 핵심 구분 포인트**
  - **데이터:** 객관적, 원천적, 정적인 사실.
  - **정보:** 주관적(맥락 포함), 목적 지향적, 동적인 가치.
- **④ 시험 출제 포인트 (서술형/객관식)**
  - **키워드:** "맥락(Context)"이 포함되면 정보, "사실(Fact)"에 머무르면 데이터.
  - DIKW 단계별 예시를 주고 어떤 단계인지 맞히는 문제 빈출.

------

### 1.1.2 데이터베이스의 정의와 특징 (Database Definition)

- **① 개념 정의**
  - 조직의 데이터를 **통합(Integrated), 저장(Stored), 운영(Operational), 공용(Shared)** 하여 관리하는 데이터의 집합체.
- **② 주요 특징 (4대 특징)**
  1. **통합된 데이터 (Integrated Data):** 중복을 최소화하여 데이터 불일치 방지.
  2. **저장된 데이터 (Stored Data):** 컴퓨터가 접근 가능한 매체에 저장된 디지털 데이터.
  3. **운영 데이터 (Operational Data):** 조직의 고유 기능을 수행하기 위해 반드시 필요한 데이터.
  4. **공용 데이터 (Shared Data):** 여러 사용자나 애플리케이션이 동시에 접근 및 공유 가능.
- **③ 데이터베이스의 특성 (성능/보안)**
  - **실시간 접근성 (Real-time Accessibility):** 쿼리에 즉각 응답.
  - **계속적인 변화 (Continuous Evolution):** 삽입, 삭제, 갱신을 통해 최신 상태 유지.
  - **내용에 의한 참조 (Content Reference):** 주소가 아닌 데이터의 값(Value)으로 참조.
- **④ OLTP vs DW 상세 비교**
  - **OLTP (Online Transaction Processing - 온라인 트랜잭션 처리):** 실시간 데이터 입력/수정 중심. 정규화(Normalization)를 통해 무결성 강조.
  - **DW (Data Warehouse - 데이터 웨어하우스):** 대량 데이터 분석/집계 중심. 비정규화(Denormalization)를 통해 조회 속도 향상.

------

### 1.1.3 데이터베이스 활용 (Operational Data Store & Data Mart)

- **① ODS (Operational Data Store - 운영 데이터 저장소)**
  - 운영 시스템과 DW 사이의 중간 단계.
  - 실시간 혹은 근실시간 데이터 통합을 담당하며, DW로 가기 전의 정제되지 않은 데이터 보관.
- **② DM (Data Mart - 데이터 마트)**
  - 특정 부서(마케팅, 재무 등)나 특정 목적을 위해 DW에서 추출한 소규모 데이터 저장소.
- **③ 시험 출제 포인트**
  - **ODS의 역할:** 운영 데이터의 통합 인터페이스 역할(실시간성 강조).
  - **계층 구조:** 운영 시스템 → ODS → DW → DM 순서 암기 필수.

------

## 1.2 데이터의 가치와 미래

### 1.2.1 빅데이터의 이해 (5V 특성)

- **① 정의:** 기존의 관리 체계로는 처리하기 힘든 거대한 데이터 집합.
- **② 5V 특성 상세**
  1. **Volume (규모):** 테라바이트(TB), 페타바이트(PB) 급의 막대한 양.
  2. **Velocity (속도):** 데이터의 생성 및 처리 속도가 실시간에 가까움.
  3. **Variety (다양성):** 정형(DB), 반정형(Log, XML), 비정형(이미지, 영상)의 공존.
  4. **Veracity (정확성/신뢰성):** 데이터의 품질과 신뢰도 확보.
  5. **Value (가치):** 분석을 통해 비즈니스 가치를 창출해야 함.
- **③ 시험 포인트:** 초기 3V(Volume, Velocity, Variety)와 추가된 요소(Veracity, Value)를 구분하는 문제.

------

### 1.2.4 위기 요인과 통제 방안 (Risk & Control)

- **① 사생활 침해 (Privacy Violation):**
  - **통제방안:** 익명화(Anonymization), 가명화(Pseudonymization). 개인정보 비식별 기술 적용.
- **② 책임 원칙의 훼손 (Accountability):** 빅데이터 예측에 의한 범죄 예방 등으로 인해 '행하지 않은 죄'로 처벌받을 위험.
  - **통제방안:** 결과 중심의 책임 강화, 알고리즘에 대한 소명 기회 제공.
- **③ 데이터 오용 (Data Misuse):** 잘못된 데이터 분석으로 잘못된 결론 도출.
  - **통제방안:** 알고리즘 해석자(Algorithmist) 도입, 분석 과정의 투명성 확보.

------

## 1.3 가치 창출을 위한 데이터 사이언스와 전략 인사이트

### 1.3.1 데이터 사이언스 (Data Science)

- **① 정의:** 데이터로부터 지식을 추출하기 위한 과학적 방법론. 통계학, IT 기술, 도메인 지식의 융합.
- **② 데이터 사이언티스트의 역량**
  - **Hard Skill:** 수학/통계 지식, 머신러닝 알고리즘 활용 역량, IT 기술(Python, SQL).
  - **Soft Skill:** 통찰력 있는 분석, 커뮤니케이션 능력, 설득력 있는 스토리텔링.

### 1.3.2 전략 인사이트 (Strategic Insight)

- **중요 포인트:** "얼마나 많은 데이터를 가졌느냐"보다 **"어떤 시각으로 분석하여 어떤 전략적 행동(Action)을 이끌어내느냐"**가 핵심 가치임.

### 1.3.3 빅데이터와 데이터 사이언스의 미래

#### ① 개념 정의

빅데이터와 데이터 사이언스의 미래는 단순한 기술적 진보를 넘어, 인공지능이 내린 결정에 대한 **윤리적 책임**을 강조하고, 분석의 전 과정을 기계가 스스로 수행하는 **지능형 자동화**로 진화하는 단계이다.

#### ② 책임 있는 AI (Responsible AI)

- **개념:** AI 모델이 공정하고(Fair), 투명하며(Transparent), 설명 가능해야(Explainable) 한다는 원칙이다.
- **배경:** AI의 판단이 대출 심사, 채용, 범죄 예측 등 인간의 삶에 직접적인 영향을 미치면서 '블랙박스(Black Box)' 알고리즘에 대한 불신이 증가함.
- **핵심 요소:**
  - **설명 가능성 (XAI, Explainable AI):** AI가 왜 그런 결과를 도출했는지 인간이 이해할 수 있도록 근거를 제공하는 기술.
  - **편향성 제어:** 학습 데이터에 포함된 인종, 성별 등의 편향이 결과에 반영되지 않도록 관리.
- **시험 출제 포인트:** "설명 가능한 AI(XAI)의 필요성"이나 "데이터 사이언티스트의 윤리적 책임"에 관한 서술형 문제.

#### ③ 자동화 분석 (Automated Analytics / AutoML)

- **개념:** 데이터 전처리, 피처 엔지니어링(Feature Engineering), 모델 선택, 하이퍼파라미터 튜닝 등 분석의 복잡한 과정을 알고리즘이 자동으로 수행하는 기술.
- **주요 기술:**
  - **AutoML (Automated Machine Learning):** 최적의 모델을 자동으로 탐색하고 최적화함.
  - **MLOps (Machine Learning Operations):** 모델의 배포, 모니터링, 재학습 과정을 자동화하여 운영 효율을 극대화함.
- **배경 및 필요성:** 데이터 사이언티스트의 부족 현상을 해결하고, 분석의 재현성(Reproducibility)과 생산성을 높이기 위함.
- **시험 출제 포인트:** "AutoML이 데이터 사이언티스트의 역할을 어떻게 변화시키는가?" (단순 반복 작업은 자동화되고, 사이언티스트는 도메인 해석과 전략 기획에 집중하게 됨)

#### ④ 핵심 구분 포인트

- **과거:** 기술적 정확도(Accuracy) 최우선.
- **미래:** 신뢰성(Trust), 윤리(Ethics), 효율성(Automation)의 균형.

#### ⑤ 시험용 핵심 문장

> 미래의 데이터 사이언스는 분석의 '결과'만큼이나 그 결과에 이르는 '과정의 투명성'과 '운영의 효율성'을 중요시한다.

------

### 💡 1과목 마무리 학습 가이드

지금까지 정리한 **1.1부터 1.3**까지의 내용은 ADP 필기에서 전체 점수의 기초를 잡아주는 구간입니다.

- **용어 풀네임 확인:** XAI(Explainable AI), AutoML(Automated Machine Learning), MLOps(Machine Learning Operations)를 꼭 기억하세요.
- **서술형 대비:** "빅데이터 위기 요인과 책임 있는 AI의 관계"를 엮어서 생각해보는 연습이 필요합니다.

이제 **1과목** 정리가 아주 완벽해졌습니다. 이 형식대로 **2과목 데이터 처리 기술 이해**의 첫 번째 챕터인 **2.1 데이터 마스터 플랜**부터 다시 시작해 볼까요? 원하시면 바로 다음 내용을 상세히 풀어드리겠습니다.

------

# 2. 데이터 처리 기술 및 분석 기획 

## 2.1 데이터 거버넌스 (Data Governance)

- **정의:** 기업 내부의 데이터 품질, 보안, 관리를 위한 전사적 관리 체계.
- **구성 3요소:** 원칙(Principle), 조직(Organization), 프로세스(Process).
- **관리 항목:** 데이터 표준화, 메타데이터 관리, 데이터 품질 관리, 데이터 보안.
- **시험 포인트:** 데이터 거버넌스는 단순한 IT 기술이 아닌 **비즈니스 목적 달성을 위한 전략적 체계**임을 강조해야 함.

## 2.2 데이터 통합 및 추출 기술 (ETL, CDC)

#### ① ETL (Extract, Transform, Load)

- **프로세스:** 소스 시스템에서 추출(E) → 스테이징 영역에서 변환(T) → 대상 DW/DM에 적재(L).
- **장점:** 정제된 고품질 데이터 확보 가능.
- **단점:** 대용량 데이터 처리 시 시간 소요(Batch 중심), 실시간성 부족.

#### ② CDC (Change Data Capture) - ⭐ 서술형 단골

- **정의:** 소스 DB의 변경 사항(I/U/D)을 실시간으로 감지하여 반영하는 기술.
- **구현 기법 비교:**
  - **Log-based:** DB 트랜잭션 로그(Redo/Archive Log)를 직접 읽음. **운영 DB 부하 최소**, Delete 감지 가능. (가장 우수)
  - **Timestamp:** 수정 시간 컬럼 활용. 구현은 쉬우나 **Delete 감지 불가**, 운영 DB 부하 존재.
  - **Trigger:** 이벤트 발생 시 자동 기록. 실시간성은 좋으나 **운영 DB 성능 저하(Overhead)**가 매우 심함.
- **시험 포인트:** "Log-based CDC가 선호되는 이유"를 성능 부하와 데이터 정합성 측면에서 서술할 수 있어야 함.

## 2.3 분산 저장 및 가공 기술

#### ① 데이터 레이크 (Data Lake) vs 데이터 웨어하우스 (DW)

- **DW:** 구조화된 데이터 중심(Schema-on-write), 분석 목적이 명확함, 주로 SQL 활용.
- **Data Lake:** 모든 형태(정형/비정형)의 Raw Data 저장(Schema-on-read), 분석 유연성 높음.

#### ② 분산 파일 시스템 (HDFS)

- **특징:** 저사양 서버를 묶어 대용량 파일 저장. 데이터 블록 복제(Default 3개)를 통해 **결함 허용(Fault Tolerance)** 보장.

#### ③ 맵리듀스 (MapReduce)

- **Map:** 데이터를 쪼개서 분산 처리.
- **Reduce:** 처리된 결과를 하나로 합침.
- **시험 포인트:** 분산 처리의 핵심은 네트워크 부하를 줄이기 위해 **데이터가 있는 곳으로 코드를 보내는 것(Data Locality)**임.

## 2.4 데이터 분석 기획 (방법론 심화)

- **KDD vs CRISP-DM 비교:**
  - **KDD:** 단계적 수행 중심 (데이터 선택-전처리-변환-마이닝-평가).
  - **CRISP-DM:** 비즈니스 중심, 단계 간 **반복과 피드백** 강조 (비즈니스 이해-데이터 이해-준비-모델링-평가-전개).
- **분석 과제 발굴:**
  - **하향식(Top-Down):** 문제가 정의되어 있고 해결책을 찾는 방식.
  - **상향식(Bottom-Up):** 데이터를 먼저 탐색하여 새로운 문제(인사이트)를 발견하는 방식 (Design Thinking과 유사).
- **분석 거버넌스:** 분석 조직 구성(집중 구조 vs 기능 구조 vs 분산 구조)의 장단점 비교.

------

## 📝 2과목 서술형 핵심 키워드

1. **분석 마스터 플랜 우선순위 결정:** 전략적 중요도(Value), 실행 용이성(Volume)을 기준으로 사분면 분석을 통해 결정.
2. **개인정보 비식별화 기술:** 가명처리, 총계처리, 데이터 마스킹, 범주화, 데이터 견본 추출 등.
3. **빅데이터 5V:** Volume(규모), Velocity(속도), Variety(다양성) + Veracity(정확성), Value(가치).

------

# 3. 데이터 분석 (⭐ 서술형 집중 공략)

## 3.1 선형 회귀 분석 (Linear Regression)

- **회귀 분석의 5가지 기본 가정:**
  1. **선형성:** 독립변수와 종속변수가 선형 관계.
  2. **독립성:** 잔차 간 상관 없음 (Durbin-Watson 지수 체크).
  3. **등분산성:** 모든 구간에서 잔차 분산 일정.
  4. **비상관성:** 잔차와 독립변수 간 상관 없음.
  5. **정상성(정규성):** 잔차가 정규분포를 따름 (Q-Q Plot 확인).
- **주요 수식:**
  - **결정계수 ($R^2$):** $1 - \frac{SSE}{SST}$ (모델 설명력).
  - **다중공선성:** **VIF > 10**이면 변수 제거 혹은 PCA 필요.

## 3.2 분류 분석과 로지스틱 회귀

- **로지스틱 회귀 수식:**
  - **Logit Function:** $Logit(P) = \ln(\frac{P}{1-P}) = \beta_0 + \beta_1 X$
  - **Odds Ratio:** $e^{\beta_1}$ ($X$가 1단위 증가 시 성공 확률 변화 배수).
- **평가 지표 (혼동 행렬):**
  - **정밀도(Precision):** $\frac{TP}{TP + FP}$ (예측 긍정 중 실제 긍정).
  - **재현율(Recall):** $\frac{TP}{TP + FN}$ (실제 긍정 중 예측 성공).
  - **F1-Score:** $2 \times \frac{Precision \times Recall}{Precision + Recall}$ (조화평균).

## 3.3 앙상블 기법 (Ensemble)

- **배깅(Bagging):** 병렬 학습. **분산(Variance)** 감소 목적 (예: Random Forest).
- **부스팅(Boosting):** 오답 가중치를 이용한 순차 학습. **편향(Bias)** 감소 목적 (예: XGBoost, LightGBM).

## 3.4 차원 축소 및 시계열

- **PCA (주성분 분석):** 상관관계가 높은 변수들을 선형 결합하여 차원 축소.
  - *시험 포인트:* 고유값(Eigenvalue) 1 이상인 지점까지 주성분 채택.
- **시계열 분석 (ARIMA):**
  - **정상성 조건:** 평균·분산 일정, 공분산은 시차에만 의존.
  - **ARIMA(p, d, q):** $p$는 AR 차수(PACF 절단), $d$는 차분 횟수, $q$는 MA 차수(ACF 절단).



------

# 4. 정형 및 비정형 데이터 마이닝 (심화 상세본)

## 4.1 데이터 마이닝 개요

- **정의:** 대규모 데이터 속에서 통계적, 수학적 기법을 통해 숨겨진 패턴과 규칙을 발견하고 이를 의사결정에 활용하는 과정.
- **학습 유형:**
  - **지도 학습(Supervised):** 레이블(정답)이 있는 데이터. (분류, 회귀)
  - **비지도 학습(Unsupervised):** 레이블이 없는 데이터. (군집, 연관 분석, PCA)

## 4.2 분류 분석 (Classification)

- **로지스틱 회귀 (Logistic Regression):**
  - **핵심:** 종속변수가 범주형(0 또는 1)인 경우 사용.
  - **수식:** $\text{logit}(p) = \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X$
  - **오즈비(Odds Ratio):** 독립변수 1단위 증가 시 성공 확률이 $e^{\beta_1}$ 배 증가함.
- **의사결정나무 (Decision Tree):**
  - **분할 기준(불순도):** * **지니 지수(Gini):** $1 - \sum p_i^2$ (작을수록 순수도가 높음)
    - **엔트로피(Entropy):** $-\sum p_i \log p_i$ (정보 획득량 최대화 방향으로 분할)
  - **장점:** 해석이 용이(White Box), 비모수적 모형.
  - **단점:** 과적합(Overfitting) 발생 가능성 높음 -> **가지치기(Pruning)** 필수.
- **랜덤 포레스트 (Random Forest):**
  - **원리:** 여러 개의 의사결정나무를 배깅(Bagging) 기법으로 결합.
  - **장점:** 무작위성 도입을 통해 **분산(Variance)을 감소**시키고 일반화 성능을 높임.

## 4.3 군집 분석 (Clustering)

- **K-Means 군집:**
  - **과정:** K개 중심점 설정 → 거리 기반 군집 할당 → 중심점 재계산 → 반복.
  - **특징:** 거리 기반(유클리드) 알고리즘. K값을 사전에 정해야 함(**Elbow Method** 활용).
- **계층적 군집 (Hierarchical):**
  - **방법:** 응집형(Bottom-up) vs 분할형(Top-down).
  - **덴드로그램(Dendrogram):** 군집 간의 거리를 시각화하여 적절한 군집 수를 결정.
- **혼합 분포 군집 (EM 알고리즘):**
  - 데이터가 가우시안 분포 등 여러 개의 확률 분포에서 생성되었다고 가정하고 확률적으로 군집 할당.

## 4.4 연관 분석 (Association Analysis)

- **알고리즘:** Apriori 알고리즘 (빈번 아이템셋 추출).
- **3대 평가지표 (서술형/계산형 0순위):**
  1. **지지도(Support):** $P(A \cap B)$ -> 전체 거래 중 A와 B가 동시에 포함된 비율.
  2. **신뢰도(Confidence):** $P(B|A) = \frac{P(A \cap B)}{P(A)}$ -> A를 샀을 때 B도 살 확률.
  3. **향상도(Lift):** $\frac{P(B|A)}{P(B)}$ -> 품목 간의 통계적 독립성 확인.
     - **Lift > 1:** 양의 상관관계 (의미 있는 규칙)
     - **Lift = 1:** 독립 (관계 없음)
     - **Lift < 1:** 음의 상관관계

## 4.5 비정형 데이터 마이닝 (텍스트 마이닝)

- **텍스트 전처리:** 토큰화(Tokenization), 불용어 제거(Stopword Removal), 어간 추출(Stemming), 표제어 추출(Lemmatization).
- **분석 기법:**
  - **TF-IDF:** 단어의 빈도(TF)와 역문서 빈도(IDF)를 사용하여 단어의 특성치를 가중화함. 특정 문서 내에서만 자주 나오는 단어일수록 중요도가 높음.
  - **감성 분석(Sentiment Analysis):** 텍스트에 나타난 주관적 의견, 감정(긍정/부정)을 추출.
  - **토픽 모델링(LDA):** 문서 집합에서 잠재적인 토픽을 확률적으로 찾아냄.
- **사회연결망 분석 (SNA):**
  - **구성:** 노드(개체)와 엣지(관계).
  - **중심성 지표:** 연결 중심성, 근접 중심성, 매개 중심성, 위세 중심성.

------

## 📝 4과목 실전 서술형 키워드 (최종 체크)

1. **데이터 불균형 해결 기법:** **SMOTE** (소수 클래스 데이터를 인접 데이터를 이용해 합성하여 증강).
2. **군집 성능 평가:** **실루엣(Silhouette) 계수**. 1에 가까울수록 잘 군집된 것이며, 0에 가까우면 군집 경계에 있는 것임.
3. **과적합 방지 기법:** 규제화(Ridge, Lasso), 교차 검증(K-fold), 조기 종료(Early Stopping).



------

# 5. 데이터 시각화 (상세 보강본)

## 5.1 시각화 인사이트 프로세스

- **정의:** 데이터를 단순히 그림으로 그리는 것이 아니라, **[정보 구조화 → 시각화 설계 → 시각화 구현]** 단계를 거쳐 의사결정에 필요한 인사이트를 도출하는 과정.
- **프로세스 상세:**
  1. **정보 구조화:** 메타데이터 정의 및 데이터 탐색(EDA). 분석 목적에 맞는 변수 선택.
  2. **시각화 설계:** 시각 도구(차트 종류) 및 시각 속성(색상, 크기) 결정.
  3. **시각화 구현:** 실제 도구(Python, R, Tableau 등)를 사용하여 차트 생성 및 대시보드 구성.

## 5.2 시각화 설계 원리 및 인지 법칙

- **게슈탈트 원리 (Gestalt Laws) - 서술형 단골:**
  - **근접성:** 가까이 있는 요소들을 하나의 그룹으로 인지.
  - **유사성:** 모양, 색상, 크기가 같은 요소들을 연관된 것으로 인지.
  - **연속성:** 선이나 곡선을 따라 배열된 요소들을 흐름으로 인지.
  - **폐쇄성:** 불완전한 형태라도 닫혀 있는 하나의 형상으로 인지.
- **프리텐티브 속성 (Pre-attentive Attributes):**
  - **개념:** 인지적 노력을 기울이기 전(0.2초 이내)에 뇌가 무의식적으로 감지하는 시각적 특징.
  - **속성 종류:** 길이, 너비, 방향, 크기, 색상, 강도, 곡률 등.
  - **시험 포인트:** "효과적인 시각화를 위해 프리텐티브 속성을 어떻게 활용해야 하는가?" → 핵심 정보에 색상이나 크기 변화를 주어 시각적 위계를 형성함.

## 5.3 데이터 유형별 시각화 기법 (장단점 비교)

- **시간 시각화 (Time Series):**
  - **종류:** 선그래프, 막대그래프, 영역차트, 계단식 그래프.
  - **키포인트:** 추세(Trend)와 계절성(Seasonality) 파악이 주 목적.
- **비교 시각화 (Comparison):**
  - **히트맵 (Heat Map):** 다차원 데이터를 색상으로 표현. **장점:** 패턴 파악 용이 / **단점:** 정밀한 수치 비교 어려움.
  - **체르노프 페이스:** 인간의 얼굴 표정 변수를 이용. **장점:** 여러 변수를 한눈에 비교 / **단점:** 주관적 해석 가능성 높음.
  - **스타 차트 (Spider Chart):** 여러 속성값을 방사형으로 표현. 변수가 너무 많으면 가독성 저하.
- **관계 시각화 (Relationship):**
  - **산점도 (Scatter Plot):** 두 변수 간 상관관계 확인.
  - **버블 차트:** 산점도에 데이터 크기(원 크기)를 추가하여 3가지 변수 표현.

## 5.4 시각화 전처리 수식 및 지표

- **데이터 정규화 (Normalization):** 서로 다른 척도의 데이터를 시각화할 때 필요.
  - **Min-Max Scaling:** $x_{new} = \frac{x - x_{min}}{x_{max} - x_{min}}$ (0~1 사이로 변환)
  - **Standardization (Z-score):** $z = \frac{x - \mu}{\sigma}$ (평균 0, 표준편차 1로 변환)
- **데이터 잉크 계수 (Data-Ink Ratio):**
  - **수식:** $\text{Data-Ink Ratio} = \frac{\text{데이터 표현에 사용된 잉크 양}}{\text{그래프에 사용된 전체 잉크 양}}$
  - **의미:** 불필요한 장식(격자, 화려한 배경)을 줄이고 데이터 자체에 집중하게 설계해야 함(에드워드 터프티 제안).

## 5.5 비정형 데이터 시각화

- **워드 클라우드 (Word Cloud):** 단어 빈도수를 크기로 표현. **시험 포인트:** 직관적이지만 단어 간의 문맥이나 관계를 보여주지는 못함.
- **네트워크 그래프:** 노드(Node)와 엣지(Edge)로 관계 표현. 사회연결망 분석(SNA) 결과를 시각화할 때 필수적.

------

## 📝 5과목 서술형 키포인트 (최종 암기)

1. **시각화의 목적 2가지:** 탐색적 분석(Exploratory - 인사이트 발견), 설명적 분석(Explanatory - 결과 전달).
2. **잘못된 시각화 사례:** 3D 차트(왜곡 발생), 데이터 잉크 계수가 너무 낮은 차트, 인위적인 축 조작(Y축을 0부터 시작하지 않는 경우 등).
3. **인포그래픽의 특징:** 데이터, 스토리, 디자인의 결합. 단순 시각화보다 메시지 전달력이 강함.

------

**이 보강된 내용을 포함하면 전체 1~5과목의 밸런스가 완벽해집니다.** 이제 이 모든 내용을 합쳐서 하나의 완성된 `.md` 파일로 변환하여 아이패드에 저장하시면 됩니다. 혹시 특정 차트(예: 박스플롯의 이상치 판정 기준 등)에 대해 더 자세한 수식이 필요하신가요?



------

## 📝 서술형 예상 키워드 답안 예시

1. **과적합(Overfitting) 해결:** 데이터 증강, L1/L2 규제, 교차검증(K-fold), 드롭아웃.
2. **Lasso 회귀:** $L1$ 페널티를 사용하며, 불필요한 변수의 계수를 0으로 만들어 **변수 선택**이 가능함.
3. **실루엣 계수:** 군집 분석의 평가지표. 1에 가까울수록 응집도는 높고 분리도는 좋음을 의미.
